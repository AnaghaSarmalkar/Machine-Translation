{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3-Fr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cpPD4EZ-3-Q",
        "colab_type": "text"
      },
      "source": [
        "# **NLP Assignment 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC0S2WrpE1j3",
        "colab_type": "text"
      },
      "source": [
        "- Import the necessary libraries!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5VeFYQ0jbUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from string import digits\n",
        "import string\n",
        "import itertools\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrFY1WFWE9gA",
        "colab_type": "text"
      },
      "source": [
        "- Please skip this step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GYOaS4rhjmm",
        "colab_type": "code",
        "outputId": "327d6c04-7b0f-4495-aeee-33d475af3af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVqIYqzFFZcN",
        "colab_type": "text"
      },
      "source": [
        "### **Load and clean the data**\n",
        "- We read the data file and strip the text of leading and trailing spaces, split lines according \\n in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgEzuNb3hcDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readFile(path):\n",
        "  file = open(path, mode='rt', encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  text = text.strip().split(\"\\n\")\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hAlBAF-Fq3n",
        "colab_type": "text"
      },
      "source": [
        "- The data is preprocessed by making the words lower case, removing digits and removing punctuation except for the apostrophe (') to retain any possessive nouns. \n",
        "- The words are tokenized by white spaces.\n",
        "- The list of words are stored in a list of sentences and returned.\n",
        "- A vocabulary of unique words pertaining to the corpus is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-e7o1FrjQQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanData(data):\n",
        "  cleandata=[]\n",
        "  for line in data:\n",
        "    # convert to lower case\n",
        "    line = line.lower()\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    line = line.translate(remove_digits)\n",
        "    #remove punctuation except for apostrophe\n",
        "    line = re.sub('[!#?,.:\";)(]', '', line)\n",
        "    # tokenize on white space\n",
        "    line = line.split()\n",
        "    cleandata.append(line)\n",
        "  flat_list = [item for sublist in cleandata for item in sublist]\n",
        "  vocab= set(flat_list)\n",
        "  return cleandata, vocab\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrQrcjI9GqNl",
        "colab_type": "text"
      },
      "source": [
        "- The unzipped data is stored in the same folder where this notebook has been kept.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5YX_FcaiZm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fr_path=\"/content/drive/My Drive/NLP Assignment 3/fr-en.fr\"\n",
        "fr = readFile(fr_path)\n",
        "clean_fr, vocab_fr= cleanData(fr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phTKiJppcxO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_path=\"/content/drive/My Drive/NLP Assignment 3/fr-en.en\"\n",
        "en = readFile(en_path)\n",
        "clean_en, vocab_en= cleanData(en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Ju-sFDHMjK",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "- Word translation probabilities are stored in a dictionary using the dictionary data structure. \n",
        "- The probabilities are initialized uniformly as a unit fraction of the French Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5yjlOI0iIOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trans_prob():\n",
        "  dictionary={}\n",
        "  count = {}\n",
        "  total = {}\n",
        "  for fr in vocab_fr:\n",
        "    total[fr]=0\n",
        "    for eng in vocab_en:\n",
        "      count[(eng, fr)] = 0\n",
        "      dictionary[(eng, fr)]=1/(len(vocab_fr))\n",
        "  return total, count, dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWIiIdzwP7Um",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "- The IBM model 1 has been implemented as per the pseudocode.\n",
        "- The input is taken as a set of sentence pairs.\n",
        "- The previously uniformly distributed translational probabilities are used here.\n",
        "- Count and total dictionary is initialized to 0.\n",
        "- Normalization has been computed for all sentence pairs.\n",
        "- Counts are collected and probabilities are estimated.\n",
        "- Before checking for convergence, the probabilities are rounded off to 3 decimal places.\n",
        "- This loop converges when the translational probability table stops changing which is checked in the while loop. SInce convergence takes time, I have placed another condition which converges for number of iterations specified. It is 15 here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShMrOdg0eVZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(total, count, dictionary, iterations):\n",
        "  c=0\n",
        "  old_t={}\n",
        "  # new_dic=copy.deepcopy(dictionary)\n",
        "  while(dictionary!=old_t and c<iterations):\n",
        "    print(\"Loop started: \",c)\n",
        "    old_t=copy.deepcopy(dictionary)\n",
        "    for i in range(len(clean_en)): #Sent pairs\n",
        "      for eng in clean_en[i]:\n",
        "        add=0\n",
        "        prod=1\n",
        "        sum_c=0\n",
        "        for fr in clean_fr[i]:\n",
        "          add+=dictionary[(eng,fr)]\n",
        "        for fr in clean_fr[i]:\n",
        "          count[(eng,fr)]+=dictionary[(eng,fr)]/add\n",
        "          total[fr]+=dictionary[(eng,fr)]/add\n",
        "\n",
        "    for f in vocab_fr:\n",
        "      for e in vocab_en:\n",
        "        dictionary[(e,f)]=round(count[(e,f)]/total[f],3)\n",
        "    c+=1\n",
        "  return dictionary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxASvmOFWep5",
        "colab_type": "text"
      },
      "source": [
        "- Gives out initial uniform word translation probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbaU4bdeKzBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total, count, dictionary = trans_prob()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtVhQvXwNTQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "bb8aefe6-c3d4-4220-cb0f-5edaae819ae0"
      },
      "source": [
        "iterations=15\n",
        "dictionary = model(total, count, dictionary, iterations)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop started:  0\n",
            "Loop started:  1\n",
            "Loop started:  2\n",
            "Loop started:  3\n",
            "Loop started:  4\n",
            "Loop started:  5\n",
            "Loop started:  6\n",
            "Loop started:  7\n",
            "Loop started:  8\n",
            "Loop started:  9\n",
            "Loop started:  10\n",
            "Loop started:  11\n",
            "Loop started:  12\n",
            "Loop started:  13\n",
            "Loop started:  14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8t5hYKUGeY",
        "colab_type": "text"
      },
      "source": [
        "- After the calculation of final translational probabilities, this dictionary is used to retrieve english aligned sentence for a given french sentence.\n",
        "- For a given word pair which includes the French word, the corresponding high valued english word is outputted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGs28CrwULrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alignments(clean_fr, dictionary):\n",
        "  eng_sent=[]\n",
        "  for sent in clean_fr:\n",
        "    eng=[]\n",
        "    for word in sent:\n",
        "      maxVal = 0\n",
        "      maxKey = ()\n",
        "      for key, value in dictionary.items():\n",
        "      # This will check if testValue matches any item in tuple.\n",
        "        if word == key[1]:\n",
        "          if maxVal < value:\n",
        "            maxVal = value\n",
        "            maxKey = key\n",
        "      eng.append(maxKey[0])\n",
        "    eng_sent.append(eng)\n",
        "  return eng_sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLfcyEOTJdSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alignments = alignments(clean_fr[:20], dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTE3SC2XGim",
        "colab_type": "text"
      },
      "source": [
        "- Print English alignments for 20 French sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR58piLMV5NH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f58471f6-3dfc-4ce7-e3f9-7a3ad53279c1"
      },
      "source": [
        "for i in range(20):\n",
        "  print(clean_fr[:20][i])\n",
        "  print(alignments[i])\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['reprise', 'de', 'la', 'session']\n",
            "['session', 'the', 'the', 'part-session']\n",
            "\n",
            "\n",
            "['je', 'déclare', 'reprise', 'la', 'session', 'du', 'parlement', 'européen', 'qui', 'avait', 'été', 'interrompue', 'le', 'vendredi', 'décembre', 'dernier', 'et', 'je', 'vous', 'renouvelle', 'tous', 'mes', 'vux', 'en', 'espérant', 'que', 'vous', 'avez', 'passé', 'de', 'bonnes', 'vacances']\n",
            "['i', 'enjoyed', 'session', 'the', 'part-session', 'the', 'parliament', 'european', 'which', 'in', 'been', 'enjoyed', 'the', 'friday', 'december', 'like', 'and', 'i', 'you', 'enjoyed', 'all', 'my', 'happy', 'the', 'enjoyed', 'that', 'you', 'you', 'past', 'the', 'these', 'enjoyed']\n",
            "\n",
            "\n",
            "['comme', 'vous', 'avez', 'pu', 'le', 'constater', 'le', 'grand', 'bogue', 'de', \"l'\", 'an', 'ne', \"s'\", 'est', 'pas', 'produit', 'en', 'revanche', 'les', 'citoyens', \"d'\", 'un', 'certain', 'nombre', 'de', 'nos', 'pays', 'ont', 'été', 'victimes', 'de', 'catastrophes', 'naturelles', 'qui', 'ont', 'vraiment', 'été', 'terribles']\n",
            "['as', 'you', 'you', 'was', 'the', 'that', 'the', 'the', 'bug', 'the', 'the', \"'\", 'not', 'the', 'is', 'not', 'the', 'the', 'bug', 'the', 'citizens', 'the', 'a', 'a', 'number', 'the', 'our', 'countries', 'have', 'been', 'number', 'the', 'disasters', 'natural', 'which', 'have', 'all', 'been', 'suffered']\n",
            "\n",
            "\n",
            "['vous', 'avez', 'souhaité', 'un', 'débat', 'à', 'ce', 'sujet', 'dans', 'les', 'prochains', 'jours', 'au', 'cours', 'de', 'cette', 'période', 'de', 'session']\n",
            "['you', 'you', 'have', 'a', 'debate', 'the', 'this', 'on', 'in', 'the', 'days', 'days', 'the', 'the', 'the', 'this', 'period', 'the', 'part-session']\n",
            "\n",
            "\n",
            "['en', 'attendant', 'je', 'souhaiterais', 'comme', 'un', 'certain', 'nombre', 'de', 'collègues', 'me', \"l'\", 'ont', 'demandé', 'que', 'nous', 'observions', 'une', 'minute', 'de', 'silence', 'pour', 'toutes', 'les', 'victimes', 'des', 'tempêtes', 'notamment', 'dans', 'les', 'différents', 'pays', 'de', \"l'\", 'union', 'européenne', 'qui', 'ont', 'été', 'touchés']\n",
            "['the', 'observe', 'i', 'minds', 'as', 'a', 'a', 'number', 'the', 'president', 'i', 'the', 'have', 'had', 'that', 'we', 'observe', 'a', 'minute', 'the', 'silence', 'for', 'all', 'the', 'number', 'the', 'storms', 'particularly', 'in', 'the', 'various', 'countries', 'the', 'the', 'union', 'european', 'which', 'have', 'been', 'affected']\n",
            "\n",
            "\n",
            "['je', 'vous', 'invite', 'à', 'vous', 'lever', 'pour', 'cette', 'minute', 'de', 'silence']\n",
            "['i', 'you', 'to', 'the', 'you', 'rise', 'for', 'this', 'minute', 'the', 'silence']\n",
            "\n",
            "\n",
            "['le', 'parlement', 'debout', 'observe', 'une', 'minute', 'de', 'silence']\n",
            "['the', 'parliament', 'rose', 'rose', 'a', 'minute', 'the', 'silence']\n",
            "\n",
            "\n",
            "['madame', 'la', 'présidente', \"c'\", 'est', 'une', 'motion', 'de', 'procédure']\n",
            "['madam', 'the', 'president', 'this', 'is', 'a', 'order', 'the', 'procedure']\n",
            "\n",
            "\n",
            "['vous', 'avez', 'probablement', 'appris', 'par', 'la', 'presse', 'et', 'par', 'la', 'télévision', 'que', 'plusieurs', 'attentats', 'à', 'la', 'bombe', 'et', 'crimes', 'ont', 'été', 'perpétrés', 'au', 'sri', 'lanka']\n",
            "['you', 'you', 'have', 'killings', 'by', 'the', 'press', 'and', 'by', 'the', 'television', 'that', 'number', 'killings', 'the', 'the', 'killings', 'and', 'killings', 'have', 'been', 'killings', 'the', 'sri', 'sri']\n",
            "\n",
            "\n",
            "[\"l'\", 'une', 'des', 'personnes', 'qui', 'vient', \"d'\", 'être', 'assassinée', 'au', 'sri', 'lanka', 'est', 'm', 'kumar', 'ponnambalam', 'qui', 'avait', 'rendu', 'visite', 'au', 'parlement', 'européen', 'il', 'y', 'a', 'quelques', 'mois', 'à', 'peine']\n",
            "['the', 'a', 'the', 'people', 'which', 'just', 'the', 'be', 'ponnambalam', 'the', 'sri', 'sri', 'is', 'mr', 'ponnambalam', 'very', 'which', 'in', 'ponnambalam', 'ponnambalam', 'the', 'parliament', 'european', 'to', 'there', 'has', 'few', 'months', 'the', 'just']\n",
            "\n",
            "\n",
            "['ne', 'pensez-vous', 'pas', 'madame', 'la', 'présidente', \"qu'\", 'il', 'conviendrait', \"d'\", 'écrire', 'une', 'lettre', 'au', 'président', 'du', 'sri', 'lanka', 'pour', 'lui', 'communiquer', 'que', 'le', 'parlement', 'déplore', 'les', 'morts', 'violentes', 'dont', 'celle', 'de', 'm', 'ponnambalam', 'et', 'pour', \"l'\", 'inviter', 'instamment', 'à', 'faire', 'tout', 'ce', 'qui', 'est', 'en', 'son', 'pouvoir', 'pour', 'chercher', 'une', 'réconciliation', 'pacifique', 'et', 'mettre', 'un', 'terme', 'à', 'cette', 'situation', 'particulièrement', 'difficile']\n",
            "['not', 'sri', 'not', 'madam', 'the', 'president', 'to', 'to', 'it', 'the', 'sri', 'a', 'letter', 'the', 'president', 'the', 'sri', 'sri', 'for', 'her', 'would', 'that', 'the', 'parliament', 'regret', 'the', 'sri', 'sri', 'which', 'the', 'the', 'mr', 'very', 'and', 'for', 'the', 'sri', 'to', 'the', 'to', 'this', 'this', 'which', 'is', 'the', 'his', 'at', 'for', 'to', 'a', 'sri', 'sri', 'and', 'to', 'a', 'the', 'the', 'this', 'situation', 'particularly', 'her']\n",
            "\n",
            "\n",
            "['oui', 'monsieur', 'evans', 'je', 'pense', \"qu'\", 'une', 'initiative', 'dans', 'le', 'sens', 'que', 'vous', 'venez', 'de', 'suggérer', 'serait', 'tout', 'à', 'fait', 'appropriée']\n",
            "['yes', 'mr', 'evans', 'i', 'i', 'to', 'a', 'initiative', 'in', 'the', 'be', 'that', 'you', 'type', 'the', 'type', 'it', 'this', 'the', 'the', 'appropriate']\n",
            "\n",
            "\n",
            "['si', \"l'\", 'assemblée', 'en', 'est', \"d'\", 'accord', 'je', 'ferai', 'comme', 'm', 'evans', \"l'\", 'a', 'suggéré']\n",
            "['if', 'the', 'house', 'the', 'is', 'the', 'agree', 'i', 'do', 'as', 'mr', 'evans', 'the', 'has', 'agrees']\n",
            "\n",
            "\n",
            "['madame', 'la', 'présidente', \"c'\", 'est', 'une', 'motion', 'de', 'procédure']\n",
            "['madam', 'the', 'president', 'this', 'is', 'a', 'order', 'the', 'procedure']\n",
            "\n",
            "\n",
            "['je', 'voudrais', 'vous', 'demander', 'un', 'conseil', 'au', 'sujet', 'de', \"l'\", 'article', 'qui', 'concerne', \"l'\", 'irrecevabilité']\n",
            "['i', 'like', 'you', 'ask', 'a', 'council', 'the', 'on', 'the', 'the', 'article', 'which', 'the', 'the', 'advice']\n",
            "\n",
            "\n",
            "['ma', 'question', 'porte', 'sur', 'un', 'sujet', 'qui', 'est', 'à', \"l'\", 'ordre', 'du', 'jour', 'du', 'jeudi', 'et', 'que', 'je', 'soulèverai', 'donc', 'une', 'nouvelle', 'fois']\n",
            "['my', 'question', 'something', 'on', 'a', 'on', 'which', 'is', 'the', 'the', 'agenda', 'the', 'agenda', 'the', 'thursday', 'and', 'that', 'i', 'will', 'therefore', 'a', 'new', 'again']\n",
            "\n",
            "\n",
            "['le', 'paragraphe', 'du', 'rapport', 'cunha', 'sur', 'les', 'programmes', \"d'\", 'orientation', 'pluriannuels', 'qui', 'sera', 'soumis', 'au', 'parlement', 'ce', 'jeudi', 'propose', \"d'\", 'introduire', 'des', 'sanctions', 'applicables', 'aux', 'pays', 'qui', 'ne', 'respectent', 'pas', 'les', 'objectifs', 'annuels', 'de', 'réduction', 'de', 'leur', 'flotte']\n",
            "['the', 'amsterdam', 'the', 'report', 'targets', 'on', 'the', 'programmes', 'the', 'guidance', 'targets', 'which', 'will', 'should', 'the', 'parliament', 'this', 'thursday', 'should', 'the', 'which', 'the', 'penalties', 'examination', 'to', 'countries', 'which', 'not', '-', 'not', 'the', 'objectives', 'annual', 'the', 'reduction', 'the', 'their', 'targets']\n",
            "\n",
            "\n",
            "['il', 'précise', 'que', 'cela', 'devrait', 'être', 'fait', 'malgré', 'le', 'principe', 'de', 'stabilité', 'relative']\n",
            "['to', 'says', 'that', 'this', 'should', 'be', 'the', 'despite', 'the', 'principle', 'the', 'stability', 'relative']\n",
            "\n",
            "\n",
            "['à', 'mon', 'sens', 'le', 'principe', 'de', 'stabilité', 'relative', 'est', 'un', 'principe', 'juridique', 'fondamental', 'de', 'la', 'politique', 'commune', 'de', 'la', 'pêche', 'et', 'toute', 'proposition', 'le', 'bouleversant', 'serait', 'juridiquement', 'irrecevable']\n",
            "['the', 'my', 'be', 'the', 'principle', 'the', 'stability', 'relative', 'is', 'a', 'principle', 'legal', 'and', 'the', 'the', 'policy', 'common', 'the', 'the', 'fisheries', 'and', 'to', 'proposal', 'the', 'subvert', 'it', 'subvert', 'subvert']\n",
            "\n",
            "\n",
            "['je', 'voudrais', 'savoir', 'si', \"l'\", 'on', 'peut', 'avancer', 'une', 'objection', 'de', 'ce', 'type', 'à', 'ce', 'qui', \"n'\", 'est', \"qu'\", 'un', 'rapport', 'pas', 'une', 'proposition', 'législative', 'et', 'si', 'je', 'suis', 'habilité', 'à', 'le', 'faire', 'ce', 'jeudi']\n",
            "['i', 'like', 'a', 'if', 'the', 'the', 'can', 'can', 'a', 'whether', 'the', 'this', 'type', 'the', 'this', 'which', 'not', 'is', 'to', 'a', 'report', 'not', 'a', 'proposal', 'legislative', 'and', 'if', 'i', 'i', 'whether', 'the', 'the', 'to', 'this', 'thursday']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}