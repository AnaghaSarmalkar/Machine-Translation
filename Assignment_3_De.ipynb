{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3-De.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTHgpeycMDVc",
        "colab_type": "text"
      },
      "source": [
        "- Import the necessary libraries!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNGEZVAi6z71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from string import digits\n",
        "import string\n",
        "import itertools\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcXo8VkeMHRG",
        "colab_type": "text"
      },
      "source": [
        "- Please skip this step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jIIwTnT66DD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6e2d618-575c-4b22-beb3-fdda4a03379d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEaWl6NfMKoO",
        "colab_type": "text"
      },
      "source": [
        "### **Load and clean the data**\n",
        "- We read the data file and strip the text of leading and trailing spaces, split lines according \\n in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAT7VBeU7BCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readFile(path):\n",
        "  file = open(path, mode='rt', encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  #Strip the text of leading and trailing spaces, split lines according \\n in a list.\n",
        "  text = text.strip().split(\"\\n\")\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rc219PSMOHM",
        "colab_type": "text"
      },
      "source": [
        "- The data is preprocessed by making the words lower case, removing digits and removing punctuation except for the apostrophe (') to retain any possessive nouns. \n",
        "- The words are tokenized by white spaces.\n",
        "- The list of words are stored in a list of sentences and returned.\n",
        "- A vocabulary of unique words pertaining to the corpus is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rmAFerC8vwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanData(data):\n",
        "  cleandata=[]\n",
        "  for line in data:\n",
        "    # convert to lower case\n",
        "    line = line.lower()\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    line = line.translate(remove_digits)\n",
        "    #remove punctuation except for apostrophe\n",
        "    line = re.sub('[!#?,.:\";)(]', '', line)\n",
        "    # tokenize on white space\n",
        "    line = line.split()\n",
        "    cleandata.append(line)\n",
        "  flat_list = [item for sublist in cleandata for item in sublist]\n",
        "  vocab= set(flat_list)\n",
        "  return cleandata, vocab\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZBKYBKKMXrL",
        "colab_type": "text"
      },
      "source": [
        "- The unzipped data is stored in the same folder where this notebook has been kept.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDOnFR-K8yiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "de_path=\"/content/drive/My Drive/NLP Assignment 3/de-en.de\"\n",
        "de = readFile(de_path)\n",
        "clean_de, vocab_de= cleanData(de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVklGyyn9DP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_path=\"/content/drive/My Drive/NLP Assignment 3/de-en.en\"\n",
        "en = readFile(en_path)\n",
        "clean_en, vocab_en= cleanData(en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4USDW6bMcPz",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "- Word translation probabilities are stored in a dictionary using the dictionary data structure. \n",
        "- The probabilities are initialized uniformly as a unit fraction of the French Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA1iCwcR9OMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trans_prob():\n",
        "  dictionary={}\n",
        "  count = {}\n",
        "  total = {}\n",
        "  for de in vocab_de:\n",
        "    total[de]=0\n",
        "    for eng in vocab_en:\n",
        "      count[(eng, de)] = 0\n",
        "      dictionary[(eng, de)]=1/(len(vocab_de))\n",
        "  return total, count, dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7FeQkkXRgCZ",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "- The IBM model 1 has been implemented as per the pseudocode.\n",
        "- The input is taken as a set of sentence pairs.\n",
        "- The previously uniformly distributed translational probabilities are used here.\n",
        "- Count and total dictionary is initialized to 0.\n",
        "- Normalization has been computed for all sentence pairs.\n",
        "- Counts are collected and probabilities are estimated.\n",
        "- Before checking for convergence, the probabilities are rounded off to 3 decimal places.\n",
        "- This loop converges when the translational probability table stops changing which is checked in the while loop. SInce convergence takes time, I have placed another condition which converges for number of iterations specified. It is 15 here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKSHJJsS9YoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(total, count, dictionary, iterations):\n",
        "  c=0\n",
        "  old_t={}\n",
        "  # new_dic=copy.deepcopy(dictionary)\n",
        "  while(dictionary!=old_t and c<iterations):\n",
        "    print(\"Loop started: \",c)\n",
        "    old_t=copy.deepcopy(dictionary)\n",
        "    for i in range(len(clean_en)): #Sent pairs\n",
        "      for eng in clean_en[i]:\n",
        "        add=0\n",
        "        prod=1\n",
        "        sum_c=0\n",
        "        for de in clean_de[i]:\n",
        "          add+=dictionary[(eng,de)]\n",
        "        for de in clean_de[i]:\n",
        "          count[(eng,de)]+=dictionary[(eng,de)]/add\n",
        "          total[de]+=dictionary[(eng,de)]/add\n",
        "\n",
        "    for de in vocab_de:\n",
        "      for en in vocab_en:\n",
        "        dictionary[(en,de)]=round(count[(en,de)]/total[de],3)\n",
        "    c+=1\n",
        "  return dictionary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT_AedurSCWX",
        "colab_type": "text"
      },
      "source": [
        "- After the calculation of final translational probabilities, this dictionary is used to retrieve english aligned sentence for a given german sentence.\n",
        "- For a given word pair which includes the German word, the corresponding high valued english word is outputted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA6Vcq1dGWUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alignments(clean_de, dictionary):\n",
        "  eng_sent=[]\n",
        "  for sent in clean_de:\n",
        "    eng=[]\n",
        "    for word in sent:\n",
        "      maxVal = 0\n",
        "      maxKey = ()\n",
        "      for key, value in dictionary.items():\n",
        "      # This will check if testValue matches any item in tuple.\n",
        "        if word == key[1]:\n",
        "          if maxVal < value:\n",
        "            maxVal = value\n",
        "            maxKey = key\n",
        "      eng.append(maxKey[0])\n",
        "    eng_sent.append(eng)\n",
        "  return eng_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnpSwd-GPT5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total, count, dictionary = trans_prob()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u6enCYhPggP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "6d414787-8599-45db-ae3e-cceebf5404b4"
      },
      "source": [
        "iterations=15\n",
        "dictionary = model(total, count, dictionary, iterations)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop started:  0\n",
            "Loop started:  1\n",
            "Loop started:  2\n",
            "Loop started:  3\n",
            "Loop started:  4\n",
            "Loop started:  5\n",
            "Loop started:  6\n",
            "Loop started:  7\n",
            "Loop started:  8\n",
            "Loop started:  9\n",
            "Loop started:  10\n",
            "Loop started:  11\n",
            "Loop started:  12\n",
            "Loop started:  13\n",
            "Loop started:  14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD5loaMiPmhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alignments = alignments(clean_de[:5], dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRK3JVs5XJeI",
        "colab_type": "text"
      },
      "source": [
        "- Print English alignments for 5 German sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrqFJd4JW6EV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "cfd50d48-0926-475d-e852-29c318723cfa"
      },
      "source": [
        "for i in range(5):\n",
        "  print(clean_de[:5][i])\n",
        "  print(alignments[i])\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['wiederaufnahme', 'der', 'sitzungsperiode']\n",
            "['resumption', 'the', 'you']\n",
            "\n",
            "\n",
            "['ich', 'erkläre', 'die', 'am', 'freitag', 'dem', 'dezember', 'unterbrochene', 'sitzungsperiode', 'des', 'europäischen', 'parlaments', 'für', 'wiederaufgenommen', 'wünsche', 'ihnen', 'nochmals', 'alles', 'gute', 'zum', 'jahreswechsel', 'und', 'hoffe', 'daß', 'sie', 'schöne', 'ferien', 'hatten']\n",
            "['i', 'festive', 'the', 'the', 'friday', 'the', 'december', 'festive', 'you', 'the', 'european', 'parliament', 'the', 'festive', 'like', 'you', 'would', 'everything', 'well', 'the', 'festive', 'and', 'hope', 'that', 'to', 'festive', 'festive', 'i']\n",
            "\n",
            "\n",
            "['wie', 'sie', 'feststellen', 'konnten', 'ist', 'der', 'gefürchtete', 'millenium-bug', 'nicht', 'eingetreten', 'doch', 'sind', 'bürger', 'einiger', 'unserer', 'mitgliedstaaten', 'opfer', 'von', 'schrecklichen', 'naturkatastrophen', 'geworden']\n",
            "['as', 'to', \"'\", 'were', 'is', 'the', \"'\", \"'\", 'not', \"'\", 'the', 'the', 'citizens', 'the', 'our', 'member', 'suffered', 'the', \"'\", 'disasters', 'have']\n",
            "\n",
            "\n",
            "['im', 'parlament', 'besteht', 'der', 'wunsch', 'nach', 'einer', 'aussprache', 'im', 'verlauf', 'dieser', 'sitzungsperiode', 'in', 'den', 'nächsten', 'tagen']\n",
            "['the', 'parliament', 'in', 'the', 'request', 'the', 'the', 'debate', 'the', 'days', 'this', 'you', 'in', 'the', 'next', 'days']\n",
            "\n",
            "\n",
            "['heute', 'möchte', 'ich', 'sie', 'bitten', '-', 'das', 'ist', 'auch', 'der', 'wunsch', 'einiger', 'kolleginnen', 'und', 'kollegen', '-', 'allen', 'opfern', 'der', 'stürme', 'insbesondere', 'in', 'den', 'verschiedenen', 'ländern', 'der', 'europäischen', 'union', 'in', 'einer', 'schweigeminute', 'zu', 'gedenken']\n",
            "['today', 'like', 'i', 'to', 'ask', '-', 'the', 'is', 'also', 'the', 'request', 'the', 's', 'and', 'mr', '-', 'all', 'terrible', 'the', 'terrible', 'of', 'in', 'the', 'various', 'countries', 'the', 'european', 'union', 'in', 'the', 'silence', 'to', 'terrible']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}